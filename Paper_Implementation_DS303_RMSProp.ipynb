{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrakritiShetty/DS303_Project_Paper_Implementation/blob/main/Paper_Implementation_DS303_RMSProp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htXt9HcO3B1M"
      },
      "source": [
        "Dataset Downloading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3JWPsdNPdZ1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import albumentations as A\n",
        "import sys\n",
        "\n",
        "import argparse\n",
        "import yaml\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkGwuHNC0CrZ"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "IMAGE_FORMAT = \".png\"\n",
        "# generating LR images from HR by bicubic downsampling. Average is simple avg, bicubic is weighted avg and subsampling is subsitution.\n",
        "DOWNSAMPLE_MODE = Image.BICUBIC\n",
        "COLOR_CHANNELS = 3\n",
        "HR_IMG_SIZE = (648, 648) \n",
        "UPSCALING_FACTOR = 4\n",
        "LR_IMG_SIZE = (HR_IMG_SIZE[0] // UPSCALING_FACTOR , HR_IMG_SIZE[1] // UPSCALING_FACTOR) # used // for integer division"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsch9s0x0DO1"
      },
      "outputs": [],
      "source": [
        "class DIV2K_Dataset(keras.utils.Sequence):\n",
        "   \n",
        "   # keras.utils.sequence is a data generator - used in situations like when we need advanced control over sample generation or when simple data does not fit into memory and must be loaded dynamically\n",
        "    \n",
        "    def __init__(self, hr_image_folder: str, batch_size: int, set_type: str):\n",
        "        self.batch_size = batch_size\n",
        "        self.hr_image_folder = hr_image_folder\n",
        "        self.images = np.sort([\n",
        "            x for x in os.listdir(hr_image_folder) if x.endswith(IMAGE_FORMAT)\n",
        "        ])\n",
        "\n",
        "        if set_type == \"train\":\n",
        "          self.images = self.images[:-200] # 700 images for training\n",
        "        elif set_type == \"val\":\n",
        "          self.images = self.images[-200:-100] # 100 images for validation\n",
        "        else:\n",
        "          self.images = self.images[-100:] # 100 images for testing\n",
        "\n",
        "        # data augmentation\n",
        "        # done on HR images only, then LR will be made from that.\n",
        "        # for training and validation sets, data augmentation includes scaling and rotation\n",
        "        if set_type in [\"train\", \"val\"]:\n",
        "            self.transform = A.Compose(\n",
        "                [\n",
        "                    A.RandomCrop(width=HR_IMG_SIZE[0], height=HR_IMG_SIZE[1], p=1.0),\n",
        "                    A.RandomRotate90(),\n",
        "                    # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.90,rotate_limit=45, p=.75),\n",
        "                    # A.OneOf([A.augmentations.geometric.resize.RandomScale (scale_limit=0.6, interpolation=1, always_apply=False, p=0.5)],[A.augmentations.geometric.resize.RandomScale (scale_limit=0.7, interpolation=1, always_apply=False, p=0.5)][A.augmentations.geometric.resize.RandomScale (scale_limit=0.8, interpolation=1, always_apply=False, p=0.5)][A.augmentations.geometric.resize.RandomScale (scale_limit=0.9, interpolation=1, always_apply=False, p=0.5)]),\n",
        "                    # A.augmentations.geometric.resize.RandomScale (scale_limit=0.6, interpolation=1, always_apply=False, p=0.5),\n",
        "                    # A.OneOf([A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5)])\n",
        "                    A.HorizontalFlip(p=0.5)\n",
        "                    \n",
        "                ]\n",
        "            )\n",
        "        else: \n",
        "            self.transform = A.Compose(\n",
        "                [\n",
        "                    A.RandomCrop(width=HR_IMG_SIZE[0], height=HR_IMG_SIZE[1], p=1.0),\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        self.to_float = A.ToFloat(max_value=255)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)//self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        random.shuffle(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # batch of samples\n",
        "        i = idx * self.batch_size\n",
        "        batch_images = self.images[i : i + self.batch_size] # all images in that particular batch\n",
        "        batch_hr_images = np.zeros((self.batch_size,) + HR_IMG_SIZE + (COLOR_CHANNELS,))\n",
        "        batch_lr_images = np.zeros((self.batch_size,) + LR_IMG_SIZE + (COLOR_CHANNELS,))\n",
        "\n",
        "\n",
        "        for i, image in enumerate(batch_images):\n",
        "           \n",
        "            hr_image = Image.open(os.path.join(self.hr_image_folder, image))\n",
        "            hr_image = np.array(hr_image) \n",
        "\n",
        "            # because the augmentations are all applied on hr images only, we need to apply the transformations on the hr images and then downsample them to lr images  \n",
        "            hr_image_transform = self.transform(image=hr_image)[\"image\"] # converts and saves hr image as lr image\n",
        "            hr_image_transform_1 = Image.fromarray(hr_image_transform)\n",
        "            lr_image_transform_1 = hr_image_transform_1.resize(LR_IMG_SIZE, resample=DOWNSAMPLE_MODE)\n",
        "            lr_image_transform = np.array(lr_image_transform_1)\n",
        "\n",
        "            batch_hr_images[i] = self.to_float(image=hr_image_transform)[\"image\"]\n",
        "            batch_lr_images[i] = self.to_float(image=lr_image_transform)[\"image\"]\n",
        "\n",
        "        return (batch_lr_images, batch_hr_images)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLY7f4nw1RLg"
      },
      "source": [
        "Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9L1pPQUz0qG"
      },
      "outputs": [],
      "source": [
        "from keras import Sequential, initializers\n",
        "from keras.layers import Conv2D, Conv2DTranspose, InputLayer, PReLU, Activation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zqvf20b9HC7"
      },
      "outputs": [],
      "source": [
        "def create_model( d: int, s: int, m: int, input_size: tuple = LR_IMG_SIZE, upscaling_factor: int = UPSCALING_FACTOR, color_channels: int = COLOR_CHANNELS):\n",
        "    model = Sequential()\n",
        "    model.add( InputLayer( input_shape=(input_size[0], input_size[1], color_channels)))\n",
        "\n",
        "    # feature extraction\n",
        "    model.add(\n",
        "        Conv2D(\n",
        "            kernel_size = 5, # f1\n",
        "            filters = d, # n1\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=initializers.HeNormal(),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # activation func after every conv layer\n",
        "    model.add( PReLU( alpha_initializer=\"zeros\", shared_axes=[1, 2]))\n",
        "\n",
        "    # shrinking\n",
        "    model.add(\n",
        "        Conv2D(\n",
        "            kernel_size = 1,\n",
        "            filters = s,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=initializers.HeNormal(),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    model.add( PReLU( alpha_initializer=\"zeros\", shared_axes=[1, 2]))\n",
        "\n",
        "    # non linear mapping\n",
        "    for _ in range(m):\n",
        "        model.add(\n",
        "            Conv2D(\n",
        "                kernel_size = 3,\n",
        "                filters = s,\n",
        "                padding=\"same\",\n",
        "                kernel_initializer=initializers.HeNormal(),\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    model.add(PReLU(alpha_initializer=\"zeros\", shared_axes=[1, 2]))\n",
        "\n",
        "    # expanding\n",
        "    model.add(\n",
        "        Conv2D(\n",
        "            kernel_size=1, \n",
        "            filters=d, \n",
        "            padding=\"same\"\n",
        "          )\n",
        "      )\n",
        "    \n",
        "    model.add(PReLU(alpha_initializer=\"zeros\", shared_axes=[1, 2]))\n",
        "\n",
        "    # deconvolution\n",
        "    model.add(\n",
        "        Conv2DTranspose(\n",
        "            kernel_size=9,\n",
        "            filters= color_channels,\n",
        "            strides= upscaling_factor,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=initializers.RandomNormal(mean=0, stddev=0.001),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLGTtPY-rFnS"
      },
      "source": [
        "Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJyRMUV3e_E9",
        "outputId": "cec88d35-c76d-41ba-a279-ca10493ec9b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge2384ClfGCf"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/gdrive/MyDrive/DIV2K_train_HR/\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZzEOArdhluE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi4KrEWKhlPy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC1SfORfStTl"
      },
      "outputs": [],
      "source": [
        "# data_path = \"/content/data/DIV2K_train_valid_HR/\"\n",
        "\n",
        "model_d =  56\n",
        "model_s = 12\n",
        "model_m = 4\n",
        "\n",
        "lr_init =  0.001\n",
        "epochs =  500\n",
        "batch_size =  30\n",
        "steps_per_epoch =  20\n",
        "val_batch_size = 20\n",
        "validation_steps =  4\n",
        "\n",
        "weights_fn= \"/content/model_{epoch:05d}.h5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq6nhjuLthgo",
        "outputId": "a5dd082c-1548-408e-ae5d-85ffa3acee8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "20/20 [==============================] - 325s 16s/step - loss: 0.0727 - val_loss: 0.0232 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "20/20 [==============================] - 312s 15s/step - loss: 0.0375 - val_loss: 0.0183 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "20/20 [==============================] - 304s 15s/step - loss: 0.0268 - val_loss: 0.0213 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "20/20 [==============================] - 327s 16s/step - loss: 0.0279 - val_loss: 0.0175 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "20/20 [==============================] - 305s 15s/step - loss: 0.0221 - val_loss: 0.0376 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "20/20 [==============================] - 316s 16s/step - loss: 0.0209 - val_loss: 0.0194 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "20/20 [==============================] - 318s 16s/step - loss: 0.0214 - val_loss: 0.0149 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "20/20 [==============================] - 297s 15s/step - loss: 0.0200 - val_loss: 0.0184 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "20/20 [==============================] - 314s 16s/step - loss: 0.0188 - val_loss: 0.0197 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "20/20 [==============================] - 305s 15s/step - loss: 0.0192 - val_loss: 0.0204 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "20/20 [==============================] - 297s 15s/step - loss: 0.0159 - val_loss: 0.0115 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "20/20 [==============================] - 289s 14s/step - loss: 0.0162 - val_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "20/20 [==============================] - 293s 15s/step - loss: 0.0185 - val_loss: 0.0130 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "20/20 [==============================] - 306s 15s/step - loss: 0.0144 - val_loss: 0.0160 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "20/20 [==============================] - 310s 15s/step - loss: 0.0139 - val_loss: 0.0126 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "20/20 [==============================] - 288s 14s/step - loss: 0.0194 - val_loss: 0.0121 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "20/20 [==============================] - 296s 15s/step - loss: 0.0121 - val_loss: 0.0171 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "20/20 [==============================] - 306s 15s/step - loss: 0.0145 - val_loss: 0.0104 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "20/20 [==============================] - 295s 15s/step - loss: 0.0157 - val_loss: 0.0229 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "20/20 [==============================] - 307s 15s/step - loss: 0.0131 - val_loss: 0.0109 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "20/20 [==============================] - 298s 15s/step - loss: 0.0136 - val_loss: 0.0076 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "20/20 [==============================] - 290s 14s/step - loss: 0.0142 - val_loss: 0.0153 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "20/20 [==============================] - 315s 16s/step - loss: 0.0122 - val_loss: 0.0111 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "20/20 [==============================] - 293s 14s/step - loss: 0.0127 - val_loss: 0.0117 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "20/20 [==============================] - 297s 15s/step - loss: 0.0132 - val_loss: 0.0150 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "20/20 [==============================] - 307s 15s/step - loss: 0.0136 - val_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "20/20 [==============================] - 313s 16s/step - loss: 0.0122 - val_loss: 0.0187 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "20/20 [==============================] - 291s 14s/step - loss: 0.0130 - val_loss: 0.0075 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "20/20 [==============================] - 312s 15s/step - loss: 0.0116 - val_loss: 0.0100 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "20/20 [==============================] - 307s 15s/step - loss: 0.0125 - val_loss: 0.0112 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "20/20 [==============================] - 293s 14s/step - loss: 0.0115 - val_loss: 0.0094 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "20/20 [==============================] - 290s 14s/step - loss: 0.0110 - val_loss: 0.0069 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "20/20 [==============================] - 312s 15s/step - loss: 0.0108 - val_loss: 0.0062 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "20/20 [==============================] - 307s 15s/step - loss: 0.0111 - val_loss: 0.0072 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "20/20 [==============================] - 290s 14s/step - loss: 0.0115 - val_loss: 0.0088 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "20/20 [==============================] - 292s 14s/step - loss: 0.0111 - val_loss: 0.0064 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "20/20 [==============================] - 289s 14s/step - loss: 0.0115 - val_loss: 0.0104 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "20/20 [==============================] - 307s 15s/step - loss: 0.0108 - val_loss: 0.0103 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "20/20 [==============================] - 309s 15s/step - loss: 0.0099 - val_loss: 0.0134 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "20/20 [==============================] - 317s 16s/step - loss: 0.0115 - val_loss: 0.0194 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "20/20 [==============================] - 291s 14s/step - loss: 0.0104 - val_loss: 0.0092 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "20/20 [==============================] - 296s 15s/step - loss: 0.0097 - val_loss: 0.0080 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "20/20 [==============================] - 289s 14s/step - loss: 0.0104 - val_loss: 0.0087 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "20/20 [==============================] - 313s 16s/step - loss: 0.0104 - val_loss: 0.0097 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "20/20 [==============================] - 292s 14s/step - loss: 0.0105 - val_loss: 0.0087 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "20/20 [==============================] - 292s 14s/step - loss: 0.0091 - val_loss: 0.0115 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "20/20 [==============================] - 288s 14s/step - loss: 0.0099 - val_loss: 0.0100 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "20/20 [==============================] - 294s 15s/step - loss: 0.0088 - val_loss: 0.0168 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "20/20 [==============================] - 293s 15s/step - loss: 0.0114 - val_loss: 0.0099 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "20/20 [==============================] - 295s 15s/step - loss: 0.0084 - val_loss: 0.0086 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "20/20 [==============================] - 291s 14s/step - loss: 0.0105 - val_loss: 0.0085 - lr: 0.0010\n",
            "Epoch 52/500\n",
            " 2/20 [==>...........................] - ETA: 3:56 - loss: 0.0107"
          ]
        }
      ],
      "source": [
        "def train() -> None:\n",
        "\n",
        "    train_dataset = DIV2K_Dataset(\n",
        "        hr_image_folder = data_path,\n",
        "        batch_size= batch_size,\n",
        "        set_type=\"train\",\n",
        "    )\n",
        "    val_dataset = DIV2K_Dataset(\n",
        "        hr_image_folder= data_path,\n",
        "        batch_size= val_batch_size,\n",
        "        set_type=\"val\",\n",
        "    )\n",
        "\n",
        "    model = create_model(d=model_d, s=model_s, m=model_m)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.RMSprop(learning_rate=lr_init),\n",
        "        loss=\"mean_squared_error\",\n",
        "    )\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"loss\", factor=0.5, patience=20, min_lr=10e-6, verbose=1\n",
        "    )\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        min_delta=10e-6, \n",
        "        patience=40, \n",
        "        verbose=0,\n",
        "        restore_best_weights=True,\n",
        "    )\n",
        "    save = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=weights_fn,\n",
        "        monitor=\"loss\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        save_freq=\"epoch\",\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        callbacks=[reduce_lr, early_stopping, save], \n",
        "        validation_data=val_dataset,\n",
        "        validation_steps=validation_steps,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe-LDeR3JyTy"
      },
      "source": [
        "Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W0Y44TYJ1Ym"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "mkoPK0m2J0aT",
        "outputId": "8669bc62-fcfe-4850-e3ad-3ea2be64bb93"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5b85fa26f554>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ],
      "source": [
        "model = keras.models.load_model(\"/content/model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osgc1KhvJ5vz"
      },
      "outputs": [],
      "source": [
        "test_dataset = DIV2K_Dataset(\n",
        "    hr_image_folder=\"/content/data/DIV2K_train_valid_HR/\",\n",
        "    batch_size=\"val_batch_size\",\n",
        "    set_type=\"test\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG6ov2uvKQvz"
      },
      "outputs": [],
      "source": [
        "n_runs = 5\n",
        "psnrs = []\n",
        "\n",
        "for _ in range(n_runs):\n",
        "    for batch in test_dataset:\n",
        "        preds = model.predict(batch[0])\n",
        "        psnr = tf.image.psnr(batch[1], preds, max_val=1.0)\n",
        "        psnr = psnr.numpy().tolist()\n",
        "        psnrs.extend(psnr)\n",
        "\n",
        "print(\"Mean PSNR: {:.3f}\".format(np.mean(psnrs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utTykwymKTyR"
      },
      "source": [
        "Visualisations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSNIH02RKVXZ"
      },
      "outputs": [],
      "source": [
        "batch_id = 0\n",
        "batch = test_dataset.__getitem__(batch_id)\n",
        "preds = model.predict(batch[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaXULuLZKYEN"
      },
      "outputs": [],
      "source": [
        "img_id = 1\n",
        "\n",
        "plt.figure(figsize=[15, 15])\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.imshow(batch[0][img_id])\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"LR Image\")\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.imshow(batch[1][img_id])\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"HR Image\")\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.imshow(preds[img_id])\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Restored Image\")\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "lr_image = Image.fromarray(np.array(batch[0][img_id] * 255, dtype=\"uint8\"))\n",
        "lr_image_resized = lr_image.resize(HR_IMG_SIZE, resample=DOWNSAMPLE_MODE)\n",
        "plt.imshow(lr_image_resized)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Bilinear Upsampling\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}